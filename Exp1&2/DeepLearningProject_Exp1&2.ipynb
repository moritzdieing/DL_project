{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3odB2hB4P6r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCpRfUJK6vkj",
        "outputId": "38d6ccf7-b3d2-42bf-b42c-c2dc46eb4710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "dataset_name = 'CIFAR10'\n",
        "\n",
        "mean = [0.5, 0.5, 0.5]\n",
        "std = [0.5, 0.5, 0.5]\n",
        "\n",
        "# Data normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "num_tasks = 5\n",
        "\n",
        "# Load training and testing datasets\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "num_classes = 10\n",
        "classes_per_subset = num_classes // num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpY7SN-jFs5e"
      },
      "outputs": [],
      "source": [
        "def split_dataset_by_classes(dataset, classes):\n",
        "    \"\"\"\n",
        "    Split the dataset to only include samples from the specified classes.\n",
        "    Args:\n",
        "        dataset: The CIFAR-10 dataset object.\n",
        "        classes: A list of class indices to include in the subset (e.g., [0, 1, 2]).\n",
        "    Returns:\n",
        "        A Subset containing only the samples from the specified classes.\n",
        "    \"\"\"\n",
        "    targets = torch.tensor(dataset.targets)  # Get all labels as a tensor\n",
        "    # Use `torch.isin` to check if each target is in the `classes` list\n",
        "    indices = torch.where(torch.isin(targets, torch.tensor(classes)))[0]\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Create subsets for all pairs of casses\n",
        "classes_list = list(range(0, num_classes))\n",
        "class_pairs = [tuple(classes_list[i:i+classes_per_subset]) for i in range(0, len(classes_list), classes_per_subset)]  # Class pairs (e.g., (0, 1), (2, 3), ...)\n",
        "train_subsets = {pair: split_dataset_by_classes(train_dataset, pair) for pair in class_pairs}\n",
        "test_subsets = {pair: split_dataset_by_classes(test_dataset, pair) for pair in class_pairs}\n",
        "\n",
        "# Example: Access a specific subset\n",
        "# subset_0_1 = subsets[(0, 1)]\n",
        "# print(f\"Subset for classes {0, 1} size: {len(subset_0_1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgW38ZvV6voW"
      },
      "outputs": [],
      "source": [
        "# Modify ResNet18 for CIFAR\n",
        "class ResNet18ForCIFAR(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet18ForCIFAR, self).__init__()\n",
        "        self.model = resnet18(weights=None)  # Initialize ResNet18\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjust for CIFAR\n",
        "        self.model.maxpool = nn.Identity()  # Remove max pooling layer\n",
        "        self.model.fc = nn.Linear(512, num_classes)  # Adjust the final layer for CIFAR\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl3aIdBm64rx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training procedure for the resnet. Use indexes to keep track of sample\n",
        "learning speed.\n",
        "\"\"\"\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    total_num_samples = len(train_loader.dataset)\n",
        "    is_correct = torch.zeros((1, total_num_samples))\n",
        "    is_correct_idx = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch} [Training]\", leave=False)\n",
        "    for batch_idx, (indexes,(inputs, targets)) in enumerate(progress_bar):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.shape[0]\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        batch_num_samples = targets.shape[0]\n",
        "        is_correct[:, indexes] = predicted.eq(targets).cpu().float()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", accuracy=f\"{100. * correct / total:.2f}%\")\n",
        "\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    return total_loss / len(train_loader), accuracy, is_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d3G3J0O64uY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Test procedure of the resnet.\n",
        "\"\"\"\n",
        "def test(model, test_loader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Wrap DataLoader with tqdm\n",
        "    progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch} [Testing]\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.shape[0]\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Update progress bar with batch loss and accuracy\n",
        "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", accuracy=f\"{100. * correct / total:.2f}%\")\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    return total_loss / len(test_loader), accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-yzQ0ss71Uz"
      },
      "outputs": [],
      "source": [
        "# number of epochs used in our exps\n",
        "num_epochs = 10\n",
        "save_per_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quhCpq6YG5Zv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function executes a single experiment (one permutation) num_runs times.\n",
        "In this case the samples are not sorted based on learning speed.\n",
        "\"\"\"\n",
        "def run_default_exp(perm, exp_name, num_runs):\n",
        "  for run in range(num_runs):\n",
        "    model = ResNet18ForCIFAR(num_classes)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "    all_samples_learning_speed = torch.zeros(num_tasks,int(50000/num_tasks))\n",
        "\n",
        "    print(exp_name, run)\n",
        "    for task in perm:\n",
        "      train_subset = train_subsets[class_pairs[task]]\n",
        "      test_subset = test_subsets[class_pairs[task]]\n",
        "      train_loader = torch.utils.data.DataLoader(dataset=list(enumerate(train_subset)), batch_size=128, shuffle=True, num_workers=4)\n",
        "      test_loader = torch.utils.data.DataLoader(dataset=test_subset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "      # For learning speed tracking\n",
        "      total_num_samples = len(train_loader.dataset)\n",
        "      M = torch.zeros((num_epochs, total_num_samples))\n",
        "\n",
        "      print(f\"Training task {task}...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          train_loss, train_acc, is_correct = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "          M[epoch] = is_correct\n",
        "          test_loss, test_acc = test(model, test_loader, criterion, device, epoch)\n",
        "          scheduler.step()\n",
        "\n",
        "          print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "          print(epoch + 1, save_per_epochs, (epoch + 1) % (save_per_epochs))\n",
        "          if ((epoch + 1) % (save_per_epochs)) == 0:\n",
        "            torch.save(model.state_dict(), f'resnet18_{dataset_name.lower()}_task{task}_epoch{epoch}.pth')\n",
        "            torch.save(optimizer.state_dict(), f'optimizer_{dataset_name.lower()}_task{task}_epoch{epoch}.pth')\n",
        "            torch.save(scheduler.state_dict(), f'scheduler_{dataset_name.lower()}_task{task}_epoch{epoch}.pth')\n",
        "            torch.save(M, f'M_{dataset_name.lower()}_{exp_name}_task{task}_epoch{epoch}.pth')\n",
        "            print(f\"Model of task {task} saved at epoch {epoch}.\")\n",
        "      all_samples_learning_speed[task] = M.sum(axis=0)\n",
        "    torch.save(all_samples_learning_speed,f'ls_{exp_name}_{run}_default.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "collapsed": true,
        "id": "ze3c07usISMd",
        "outputId": "e99b0eb2-04cb-4d43-b9a7-f866912d2bba"
      },
      "outputs": [],
      "source": [
        "exp_permutations = [[0,1,2,3,4],[4,3,1,2,0],[1,4,3,0,2],[3,2,0,4,1],[2,0,4,1,3]]\n",
        "exp_names = ['p0','p1','p2','p3','p4']\n",
        "num_runs = 5\n",
        "\n",
        "for p,n in zip(exp_permutations,exp_names):\n",
        "  run_default_exp(p,n, num_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd7_5xk-KoSz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Compute and print learning speed results. We print the learning speed per task\n",
        "and run and also the averages per run. The total array finally contians the\n",
        "averages per experiment.\n",
        "\"\"\"\n",
        "def run_default_average(perm, exp_names, num_runs):\n",
        "  total = []\n",
        "  for idx_1, exp_name in enumerate(exp_names):\n",
        "    averages = [0] * num_runs\n",
        "    print(f\"experiment {exp_name}\")\n",
        "\n",
        "    for run in range(num_runs):\n",
        "      print(f\"Run: {run}\")\n",
        "      inner_averages = []\n",
        "      learn_speed = torch.load(f'ls_{exp_name}_{run}_default.pth')\n",
        "      count = [torch.bincount(learn_speed[i].int()) for i in range(5)]\n",
        "\n",
        "      # average learning speed:\n",
        "      for idx_2, t in enumerate(count):\n",
        "        avg = 0\n",
        "        for idx_in, el in enumerate(t):\n",
        "          avg += el * idx_in\n",
        "        print(f\"Learned Task {idx_2} (actual task {perm[idx_1][idx_2]}) avg. learning speed: {avg/t.sum()}\")\n",
        "        inner_averages.append(avg/t.sum())\n",
        "\n",
        "      # compute speed average for this run\n",
        "      averages[run] = sum(inner_averages) / len(inner_averages)\n",
        "    for pr_idx, avg in enumerate(averages):\n",
        "      print(f\"Run {pr_idx} average: {avg}\")\n",
        "\n",
        "    total.append(sum(averages) / len(averages))\n",
        "  print(total)\n",
        "      # could either store final res or connect with drive for easier use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "collapsed": true,
        "id": "a0AXbdhPNIcv",
        "outputId": "4c34d3e1-61f4-4c0b-86b3-449b27888920"
      },
      "outputs": [],
      "source": [
        "exp_permutations = [[0,1,2,3,4],[4,3,1,2,0],[1,4,3,0,2],[3,2,0,4,1],[2,0,4,1,3]]\n",
        "exp_names = ['p0','p1','p2','p3','p4']\n",
        "\n",
        "run_default_average(exp_permutations, exp_names, num_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNc5xgQ9qt0Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function executes a single experiment (one permutation) num_runs times.\n",
        "In this case the samples are sorted based on learning speed. If desc=True, in\n",
        "descending order, otherwise in ascending order\n",
        "\"\"\"\n",
        "def run_exp_ord(perm, exp_name, desc, num_runs):\n",
        "  for run in range(num_runs):\n",
        "    model = ResNet18ForCIFAR(num_classes)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "    all_samples_learning_speed = torch.zeros(num_tasks,int(50000/num_tasks))\n",
        "\n",
        "    for task in perm:\n",
        "      print(f\"Exp: {desc}, Run: {run}, Task: {task}.\")\n",
        "\n",
        "      M = torch.load(f'M_{dataset_name.lower()}_p0_task{task}_epoch{num_epochs-1}.pth') # load M files containing sample learning speed (last run of 1st exp)\n",
        "      learning_speed = M.sum(axis=0)\n",
        "      sorted_learning_speed, sorted_indices = torch.sort(learning_speed, descending=desc)\n",
        "\n",
        "      train_subset = train_subsets[class_pairs[task]]\n",
        "      test_subset = test_subsets[class_pairs[task]]\n",
        "      train_subset_ord = list(enumerate(Subset(train_dataset, [train_subset.indices[i] for i in sorted_indices]))) # sort subset using learning speed\n",
        "      train_loader = torch.utils.data.DataLoader(dataset=train_subset_ord, batch_size=128, shuffle=False, num_workers=4)\n",
        "      test_loader = torch.utils.data.DataLoader(dataset=test_subset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "      # For learning speed tracking\n",
        "      total_num_samples = len(train_loader.dataset)\n",
        "      M = torch.zeros((num_epochs, total_num_samples))\n",
        "\n",
        "      print(f\"Training task {task}...\")\n",
        "      for epoch in range(num_epochs):\n",
        "          train_loss, train_acc, is_correct = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "          M[epoch] = is_correct\n",
        "          test_loss, test_acc = test(model, test_loader, criterion, device, epoch)\n",
        "          scheduler.step()\n",
        "\n",
        "          suffix1 = \"desc\" if desc else \"asc\"\n",
        "\n",
        "          print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "          print(epoch + 1, save_per_epochs, (epoch + 1) % (save_per_epochs))\n",
        "          if ((epoch + 1) % (save_per_epochs)) == 0:\n",
        "            torch.save(model.state_dict(), f'resnet18_{dataset_name.lower()}_task{task}_epoch{epoch}_{suffix1}.pth')\n",
        "            torch.save(optimizer.state_dict(), f'optimizer_{dataset_name.lower()}_task{task}_epoch{epoch}_{suffix1}.pth')\n",
        "            torch.save(scheduler.state_dict(), f'scheduler_{dataset_name.lower()}_task{task}_epoch{epoch}_{suffix1}.pth')\n",
        "            torch.save(M, f'M_{dataset_name.lower()}_task{task}_epoch{epoch}_{suffix1}.pth')\n",
        "            print(f\"Model of task {task} saved at epoch {epoch}.\")\n",
        "      all_samples_learning_speed[task] = M.sum(axis=0)\n",
        "    torch.save(all_samples_learning_speed,f'ls_{exp_name}_{suffix1}_run{run}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "collapsed": true,
        "id": "uetrVgfHsrzb",
        "outputId": "97b513a3-2e94-40ac-c904-70f239cd6c18"
      },
      "outputs": [],
      "source": [
        "exp_permutations = [[0,1,2,3,4],[4,3,1,2,0],[1,4,3,0,2],[3,2,0,4,1],[2,0,4,1,3]]\n",
        "exp_names = ['p0','p1','p2','p3','p4']\n",
        "num_runs = 5\n",
        "\n",
        "for desc in [True, False]:\n",
        "  for p,n in zip(exp_permutations,exp_names):\n",
        "    run_exp_ord(p,n, desc, num_runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S1BhfHxsq8P"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Compute and print learning speed results. We print the learning speed per task\n",
        "and run and also the averages per run. The total array finally contians the\n",
        "averages per experiment.\n",
        "Desc parameter again indicates which sorting is used.\n",
        "\"\"\"\n",
        "def run_average(perm, exp_names, desc, num_runs):\n",
        "  total = []\n",
        "  for idx_1, exp_name in enumerate(exp_names):\n",
        "    averages = [0] * num_runs\n",
        "    print(f\"{desc}, experiment {exp_name}\")\n",
        "\n",
        "    for run in range(num_runs):\n",
        "      print(f\"Run: {run}\")\n",
        "      inner_averages = []\n",
        "      learn_speed = torch.load(f'ls_{exp_name}_{desc}_run{run}.pth')\n",
        "      count = [torch.bincount(learn_speed[i].int()) for i in range(5)]\n",
        "\n",
        "      # average learning speed:\n",
        "      for idx_2, t in enumerate(count):\n",
        "        avg = 0\n",
        "        for idx_in, el in enumerate(t):\n",
        "          avg += el * idx_in\n",
        "        print(f\"Learned Task {idx_2} (actual task {perm[idx_1][idx_2]}) avg. learning speed: {avg/t.sum()}\")\n",
        "        inner_averages.append(avg/t.sum())\n",
        "\n",
        "      # compute speed average for this run\n",
        "      averages[run] = sum(inner_averages) / len(inner_averages)\n",
        "    for pr_idx, avg in enumerate(averages):\n",
        "      print(f\"Run {pr_idx} average: {avg}\")\n",
        "\n",
        "    total.append(sum(averages) / len(averages))\n",
        "  print(total)\n",
        "      # could either store final res or connect with drive for easier use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a672YU8hiyj6"
      },
      "outputs": [],
      "source": [
        "exp_permutations = [[0,1,2,3,4],[4,3,1,2,0],[1,4,3,0,2],[3,2,0,4,1],[2,0,4,1,3]]\n",
        "exp_names = ['p0','p1','p2','p3','p4']\n",
        "order = \"desc\"\n",
        "\n",
        "run_average(exp_permutations, exp_names, order, num_runs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
